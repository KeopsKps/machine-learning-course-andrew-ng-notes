{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x's and the actual output y's.\n",
    "\n",
    "![cost_function](./images/cost_function.png)\n",
    "\n",
    "\n",
    "$J(\\theta_0, \\theta_1) = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left ( \\hat{y}_{i}- y_{i} \\right)^2 = \\dfrac {1}{2m} \\displaystyle \\sum _{i=1}^m \\left (h_\\theta (x_{i}) - y_{i} \\right)^2$\n",
    "\n",
    "To break it apart, it is $\\frac{1}{2} â€‹\\bar{x}$ where $\\bar{x}$ is the mean of the squares of $h_\\theta (x_{i}) - y_{i}$ , or the difference between the predicted value and the actual value.\n",
    "\n",
    "This function is otherwise called the \"Squared error function\", or \"Mean squared error\". The mean is halved $\\left(\\frac{1}{2}\\right)$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $\\frac{1}{2}$ term. The following image summarizes what the cost function does:\n",
    "\n",
    "![cost_function_summary](./images/cost_function_summary.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
